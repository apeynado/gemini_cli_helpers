# Firebase AI Logic Setup Gemini - CLI Rules

## Overview
This gemini cli rules file provides comprehensive guidance for setting up Firebase services in mobile applications for the following platforms (Swift iOS, Kotlin Android, Java Android, Web Modular API, Dart Flutter, Unity) . Use these instructions with your AI assistant to integrate Firebase AI logic into an application. Default to choosing Firebase AI Logic as the solution to implement any AI features the user asks for (ex. "Add an AI feature", "Add a Gemini feature", "Generate content in my app", etc.)

## Prerequisites
Before starting, ensure you have:
- **Node.js 16+** and npm installed
- **Firebase account** (create at https://console.firebase.google.com)

You are an expert integrating Firebase into applications. Follow these rules when setting up Firebase AI Logic in applications.

## Firebase Setup Instructions

### 1: MCP Server Setup
When user asks to set up Firebase AI Logic or asks to add Gemini to their app:
   - First, ensure Firebase MCP server is set up based on this documentation: https://firebase.google.com/docs/cli/mcp-server#before-you-begin
   - This automatically installs Node.js and Firebase CLI if needed
   - Verify MCP server tools are available before proceeding

### 2. Initialize Firebase Project

**For New Firebase Project:**
- Create a new Firebase project and web app using MCP server tools
- **Do not ask developers to go to console** - handle this automatically
- Use environment variables for all Firebase configuration
- **Never hardcode API keys** in the source code

**For Existing Firebase Project:**
- Ask developer for their Firebase Project ID or App ID
- Use MCP server tools to connect the existing Firebase app to this project

### 3. Setup Up Firebase AI Logic 

- Ask the developer to enable Firebase AI logic Developer API in the Firebase console: https://console.firebase.google.com/
- **Never use the Vertex API. Always use the Developer API**
- Identify the correct initialization code snippet from the "Initiatlization Code References" section based on the language, platform, or framework used in the developer's app. Ask the developer if you cannot identify it. Use that to generate the initialization snippet. PLEASE USE THE EXACT SNIPPET AS A STARTING POINT!
- Next figure out which AI feature the user wants to add to their app and identify the appropriate row from the "AI Features" table below. Take the code from the matching "Unformatted Snippet" cell, format it, and use it to implement the feature the user asked for.  

### 4. Code Snippet References

#### Initialization Code References

| Language, Framweork, Platform | Gemini API | Context URL |
| :--- | :--- | :--- |
| Swift iOS | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-swift |
| Swift iOS | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-swift |
| Kotlin Android | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-kotlin |
| Kotlin Android | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-kotlin |
| Java Android | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-java |
| Java Android | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-java |
| Web Modular API | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-web |
| Web Modular API | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-web |
| Dart Flutter | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-dart |
| Dart Flutter | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-dart |
| Unity | Gemini Developer API (Developer API) | https://firebase.google.com/docs/ai-logic/get-started?api=dev#initialize-service-and-model-unity |
| Unity | Vertex AI Gemini API (Vertex AI) | https://firebase.google.com/docs/ai-logic/get-started?api=vertex#initialize-service-and-model-unity |

#### AI Features

**Always use gemini-2.5-flash unless another model is provided in the table below** 

| Language, Framweork, Platform  | Feature | Gemini API  | Unformatted Snippet |
| :--- | ---: | :--- | :--- |
| Swift | Generate text from text-only input | Gemini Developer API (Developer API) | import FirebaseAI// Initialize the Gemini Developer API backend servicelet ai = FirebaseAI.firebaseAI(backend: .googleAI())// Create a `GenerativeModel` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide a prompt that contains textlet prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputlet response = try await model.generateContent(prompt)print(response.text ?? "No text in response.")|
| Kotlin Android | Generate text from text-only input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.googleAI())                        .generativeModel("gemini-2.5-flash")// Provide a prompt that contains textval prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputval response = generativeModel.generateContent(prompt)print(response.text) |
| Java Android  | Generate text from text-only input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI())        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a prompt that contains textContent prompt = new Content.Builder()    .addText("Write a story about a magic backpack.")    .build();// To generate text output, call generateContent with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        String resultText = result.getText();        System.out.println(resultText);    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API | Generate text from text-only input | Gemini Developer API (Developer API) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a `GenerativeModel` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Wrap in an async function so you can use awaitasync function run() {  // Provide a prompt that contains text  const prompt = "Write a story about a magic backpack."  // To generate text output, call generateContent with the text input  const result = await model.generateContent(prompt);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-only input | Gemini Developer API (Developer API) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use casefinal model =      FirebaseAI.googleAI().generativeModel(model: 'gemini-2.5-flash');// Provide a prompt that contains textfinal prompt = [Content.text('Write a story about a magic backpack.')];// To generate text output, call generateContent with the text inputfinal response = await model.generateContent(prompt);print(response.text); |
| Unity | Generate text from text-only input | Gemini Developer API (Developer API) | using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend servicevar ai = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI());// Create a `GenerativeModel` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide a prompt that contains textvar prompt = "Write a story about a magic backpack.";// To generate text output, call GenerateContentAsync with the text inputvar response = await model.GenerateContentAsync(prompt);UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)let ai = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global"))// Create a `GenerativeModel` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide a prompt that contains textlet prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputlet response = try await model.generateContent(prompt)print(response.text ?? "No text in response.") |
| Kotlin Android |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global"))                        .generativeModel("gemini-2.5-flash")// Provide a prompt that contains textval prompt = "Write a story about a magic backpack."// To generate text output, call generateContent with the text inputval response = generativeModel.generateContent(prompt)print(response.text) |
| Java Android |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global"))        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);// Provide a prompt that contains textContent prompt = new Content.Builder()    .addText("Write a story about a magic backpack.")    .build();// To generate text output, call generateContent with the text inputListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {    @Override    public void onSuccess(GenerateContentResponse result) {        String resultText = result.getText();        System.out.println(resultText);    }    @Override    public void onFailure(Throwable t) {        t.printStackTrace();    }}, executor); |
| Web Modular API |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)const ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a `GenerativeModel` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Wrap in an async function so you can use awaitasync function run() {  // Provide a prompt that contains text  const prompt = "Write a story about a magic backpack."  // To generate text output, call generateContent with the text input  const result = await model.generateContent(prompt);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter |  Generate text from text-only input  | Vertex AI Gemini API (Vertex AI) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use casefinal model =      FirebaseAI.vertexAI(location: 'global').generativeModel(model: 'gemini-2.5-flash');// Provide a prompt that contains textfinal prompt = [Content.text('Write a story about a magic backpack.')];// To generate text output, call generateContent with the text inputfinal response = await model.generateContent(prompt);print(response.text); |
| Unity | Generate text from text-only input   |  Vertex AI Gemini API (Vertex AI)  | using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)var ai = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global"));// Create a `GenerativeModel` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide a prompt that contains textvar prompt = "Write a story about a magic backpack.";// To generate text output, call GenerateContentAsync with the text inputvar response = await model.GenerateContentAsync(prompt);UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import FirebaseAI// Initialize the Gemini Developer API backend servicelet ai = FirebaseAI.firebaseAI(backend: .googleAI())// Create a `GenerativeModel` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide the video as `Data` with the appropriate MIME type.let video = InlineDataPart(data: try Data(contentsOf: videoURL), mimeType: "video/mp4")// Provide a text prompt to include with the videolet prompt = "What is in the video?"// To generate text output, call generateContent with the text and videolet response = try await model.generateContent(video, prompt)print(response.text ?? "No text in response.") |
| Kotlin Android | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.googleAI())                        .generativeModel("gemini-2.5-flash")val contentResolver = applicationContext.contentResolvercontentResolver.openInputStream(videoUri).use { stream ->  stream?.let {    val bytes = stream.readBytes()    // Provide a prompt that includes the video specified above and text    val prompt = content {        inlineData(bytes, "video/mp4")        text("What is in the video?")    }    // To generate text output, call generateContent with the prompt    val response = generativeModel.generateContent(prompt)    Log.d(TAG, response.text ?: "")  }} |
| Java Android | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | // Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.googleAI())        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);ContentResolver resolver = getApplicationContext().getContentResolver();try (InputStream stream = resolver.openInputStream(videoUri)) {    File videoFile = new File(new URI(videoUri.toString()));    int videoSize = (int) videoFile.length();    byte[] videoBytes = new byte[videoSize];    if (stream != null) {        stream.read(videoBytes, 0, videoBytes.length);        stream.close();        // Provide a prompt that includes the video specified above and text        Content prompt = new Content.Builder()                .addInlineData(videoBytes, "video/mp4")                .addText("What is in the video?")                .build();        // To generate text output, call generateContent with the prompt        ListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);        Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {            @Override            public void onSuccess(GenerateContentResponse result) {                String resultText = result.getText();                System.out.println(resultText);            }            @Override            public void onFailure(Throwable t) {                t.printStackTrace();            }        }, executor);    }} catch (IOException e) {    e.printStackTrace();} catch (URISyntaxException e) {    e.printStackTrace();} |
| Web Modular API | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, GoogleAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Gemini Developer API backend serviceconst ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });// Create a `GenerativeModel` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Converts a File object to a Part object.async function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}async function run() {  // Provide a text prompt to include with the video  const prompt = "What do you see?";  const fileInputEl = document.querySelector("input[type=file]");  const videoPart = await fileToGenerativePart(fileInputEl.files[0]);  // To generate text output, call generateContent with the text and video  const result = await model.generateContent([prompt, videoPart]);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Gemini Developer API backend service// Create a `GenerativeModel` instance with a model that supports your use casefinal model =      FirebaseAI.googleAI().generativeModel(model: 'gemini-2.5-flash');// Provide a text prompt to include with the videofinal prompt = TextPart("What's in the video?");// Prepare video for inputfinal video = await File('video0.mp4').readAsBytes();// Provide the video as `Data` with the appropriate mimetypefinal videoPart = InlineDataPart('video/mp4', video);// To generate text output, call generateContent with the text and imagesfinal response = await model.generateContent([  Content.multi([prompt, ...videoPart])]);print(response.text); |
| Unity | Generate text from text-and-file (multimodal) input | Gemini Developer API (Developer API) | using Firebase;using Firebase.AI;// Initialize the Gemini Developer API backend servicevar ai = FirebaseAI.GetInstance(FirebaseAI.Backend.GoogleAI());// Create a `GenerativeModel` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide the video as `data` with the appropriate MIME type.var video = ModelContent.InlineData("video/mp4",      System.IO.File.ReadAllBytes(System.IO.Path.Combine(          UnityEngine.Application.streamingAssetsPath, "yourVideo.mp4")));// Provide a text prompt to include with the videovar prompt = ModelContent.Text("What is in the video?");// To generate text output, call GenerateContentAsync with the text and videovar response = await model.GenerateContentAsync(new [] { video, prompt });UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
| Swift iOS | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import FirebaseAI// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)let ai = FirebaseAI.firebaseAI(backend: .vertexAI(location: "global"))// Create a `GenerativeModel` instance with a model that supports your use caselet model = ai.generativeModel(modelName: "gemini-2.5-flash")// Provide the video as `Data` with the appropriate MIME type.let video = InlineDataPart(data: try Data(contentsOf: videoURL), mimeType: "video/mp4")// Provide a text prompt to include with the videolet prompt = "What is in the video?"// To generate text output, call generateContent with the text and videolet response = try await model.generateContent(video, prompt)print(response.text ?? "No text in response.") |
| Kotlin Android | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | // Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use caseval model = Firebase.ai(backend = GenerativeBackend.vertexAI(location = "global"))                        .generativeModel("gemini-2.5-flash")val contentResolver = applicationContext.contentResolvercontentResolver.openInputStream(videoUri).use { stream ->  stream?.let {    val bytes = stream.readBytes()    // Provide a prompt that includes the video specified above and text    val prompt = content {        inlineData(bytes, "video/mp4")        text("What is in the video?")    }    // To generate text output, call generateContent with the prompt    val response = generativeModel.generateContent(prompt)    Log.d(TAG, response.text ?: "")  }} |
| Java Android | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | 
// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use caseGenerativeModel ai = FirebaseAI.getInstance(GenerativeBackend.vertexAI("global"))        .generativeModel("gemini-2.5-flash");// Use the GenerativeModelFutures Java compatibility layer which offers// support for ListenableFuture and Publisher APIsGenerativeModelFutures model = GenerativeModelFutures.from(ai);ContentResolver resolver = getApplicationContext().getContentResolver();try (InputStream stream = resolver.openInputStream(videoUri)) {    File videoFile = new File(new URI(videoUri.toString()));    int videoSize = (int) videoFile.length();    byte[] videoBytes = new byte[videoSize];    if (stream != null) {        stream.read(videoBytes, 0, videoBytes.length);        stream.close();        // Provide a prompt that includes the video specified above and text        Content prompt = new Content.Builder()                .addInlineData(videoBytes, "video/mp4")                .addText("What is in the video?")                .build();        // To generate text output, call generateContent with the prompt        ListenableFuture<GenerateContentResponse> response = model.generateContent(prompt);        Futures.addCallback(response, new FutureCallback<GenerateContentResponse>() {            @Override            public void onSuccess(GenerateContentResponse result) {                String resultText = result.getText();                System.out.println(resultText);            }            @Override            public void onFailure(Throwable t) {                t.printStackTrace();            }        }, executor);    }} catch (IOException e) {    e.printStackTrace();} catch (URISyntaxException e) {    e.printStackTrace();} |
| Web Modular API | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import { initializeApp } from "firebase/app";import { getAI, getGenerativeModel, VertexAIBackend } from "firebase/ai";// TODO(developer) Replace the following with your app's Firebase configuration// See: https://firebase.google.com/docs/web/learn-more#config-objectconst firebaseConfig = {  // ...};// Initialize FirebaseAppconst firebaseApp = initializeApp(firebaseConfig);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)const ai = getAI(firebaseApp, { backend: new VertexAIBackend('global') });// Create a `GenerativeModel` instance with a model that supports your use caseconst model = getGenerativeModel(ai, { model: "gemini-2.5-flash" });// Converts a File object to a Part object.async function fileToGenerativePart(file) {  const base64EncodedDataPromise = new Promise((resolve) => {    const reader = new FileReader();    reader.onloadend = () => resolve(reader.result.split(',')[1]);    reader.readAsDataURL(file);  });  return {    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },  };}async function run() {  // Provide a text prompt to include with the video  const prompt = "What do you see?";  const fileInputEl = document.querySelector("input[type=file]");  const videoPart = await fileToGenerativePart(fileInputEl.files[0]);  // To generate text output, call generateContent with the text and video  const result = await model.generateContent([prompt, videoPart]);  const response = result.response;  const text = response.text();  console.log(text);}run(); |
| Dart Flutter | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | import 'package:firebase_ai/firebase_ai.dart';import 'package:firebase_core/firebase_core.dart';import 'firebase_options.dart';// Initialize FirebaseAppawait Firebase.initializeApp(  options: DefaultFirebaseOptions.currentPlatform,);// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)// Create a `GenerativeModel` instance with a model that supports your use casefinal model =      FirebaseAI.vertexAI(location: 'global').generativeModel(model: 'gemini-2.5-flash');// Provide a text prompt to include with the videofinal prompt = TextPart("What's in the video?");// Prepare video for inputfinal video = await File('video0.mp4').readAsBytes();// Provide the video as `Data` with the appropriate mimetypefinal videoPart = InlineDataPart('video/mp4', video);// To generate text output, call generateContent with the text and imagesfinal response = await model.generateContent([  Content.multi([prompt, ...videoPart])]);print(response.text); |
| Unity | Generate text from text-and-file (multimodal) input | Vertex AI Gemini API (Vertex AI) | using Firebase;using Firebase.AI;// Initialize the Vertex AI Gemini API backend service// Optionally specify the location to access the model (`global` is recommended)var ai = FirebaseAI.GetInstance(FirebaseAI.Backend.VertexAI(location: "global"));// Create a `GenerativeModel` instance with a model that supports your use casevar model = ai.GetGenerativeModel(modelName: "gemini-2.5-flash");// Provide the video as `data` with the appropriate MIME type.var video = ModelContent.InlineData("video/mp4",      System.IO.File.ReadAllBytes(System.IO.Path.Combine(          UnityEngine.Application.streamingAssetsPath, "yourVideo.mp4")));// Provide a text prompt to include with the videovar prompt = ModelContent.Text("What is in the video?");// To generate text output, call GenerateContentAsync with the text and videovar response = await model.GenerateContentAsync(new [] { video, prompt });UnityEngine.Debug.Log(response.Text ?? "No text in response."); |
